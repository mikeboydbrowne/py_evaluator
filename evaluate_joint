#!/usr/bin/env python
from __future__ import division
import argparse # optparse is deprecated
import math
from itertools import islice # slicing for iterators

#### BLEU CODE

# calculates the n-gram score for the sentence
def ngram_calc(h, ref, val):
    num_grams           = len(ref) - val + 1
    num_matches         = 0.0
    num_matches_inter   = 0

    for i in range(0, num_grams):
        if ngram_in(ref[i:i + val], h) == 1:
            num_matches = num_matches + 1
    return num_matches / num_grams

# returns whether or not the n-gram is in the sentence
def ngram_in(ref, h):
    num_grams  = len(h) - len(ref) + 1
    for i in range(0, num_grams, + 1):
        if ref == h[i: i + len(ref)]:
            return 1
    return 0

# calculates the BLEU score
def bleu_calc(h, ref):

    # calculating brevity penalty
    bp = brevity_penalty(h, ref)

    # calculating the precisions of all n-gram translations
    precisions = []
    for i in range(1, len(ref)):
        precisions.append(ngram_calc(h, ref, i))

    # calculating the bleu score
    ret_val = 0.0
    for p in precisions:
        if p > 0.0:
            ret_val = ret_val + p
    return bp * ret_val

# calculates the brevity penalty
def brevity_penalty(h, ref):
    ret_val = 0.0
    if len(h) < len(ref):
        ret_val = 1.0
    else:
        ret_val = pow(math.e, (1.0 - len(ref)/len(h)))
    return ret_val

#### METEOR CODE

# initializing tunable variables
a = .55
b = .9
l = .05

# creating a length function for iterators
def len_iterable(i):
    return sum(1 for _ in i)

# calculate precision
def precision(h, ref):
    if word_matches(h, ref) != 0:
        return (word_matches(h, ref) / len(h))
    else:
        return 0.0;

# calculate recall
def recall(h, ref):
    if word_matches(h, ref) != 0:
        return (word_matches(h, ref) / len(ref))
    else:
        return 0.0

# calcualtes the penalty
def penalty (h, ref):

    # print pow((chunks(h, ref) / word_matches(h, ref)), b)
    if word_matches(h, ref) != 0:
        return (1 - l * pow((chunks(h, ref) / word_matches(h, ref)), b))
    else:
        return 0.0

# calculates the number of chunks in the sentence
def chunks (h, ref):
    # initializing variables
    num_chunks      = 0.0
    length          = 0.0
    pos             = 0
    chunk_started   = 0.0

    # list converation
    h_list = list(h)
    ref_list = list(ref)

    if len(h) < len(ref):
        length = len(h)
    else:
        length = len(ref)

    if word_matches(h, ref) == len(ref) == len(h):
        num_chunks = len(ref)
        return num_chunks

    while length > pos:
        if ref_list[pos] == h_list[pos]:
            if chunk_started == 0:
                chunk_started = 1
        else:
            if chunk_started:
                num_chunks = num_chunks + 1.0

        # incrementing the position
        pos = pos + 1

    # returning the number of chunks
    return num_chunks

# calculates the METEOR value of the reference and translation
def meteor_calc(h, ref):
    if recall(h, ref) == penalty(h, ref) == 0.0:
        return 0.0
    else:
        return (penalty(h, ref) * ((precision(h, ref) * recall(h, ref)) / ((1.0 - a) * (precision(h, ref) + recall(h, ref)))))

# calculates the number of word matches between the reference and translation
def word_matches(h, ref):
    return sum(1 for w in h if w in ref)

def main():
    parser = argparse.ArgumentParser(description='Evaluate translation hypotheses.')
    parser.add_argument('-i', '--input', default='data/hyp1-hyp2-ref',
            help='input file (default data/hyp1-hyp2-ref)')
    parser.add_argument('-n', '--num_sentences', default=None, type=int,
            help='Number of hypothesis pairs to evaluate')
    # note that if x == [1, 2, 3], then x[:None] == x[:] == x (copy); no need for sys.maxint
    opts = parser.parse_args()

    # we create a generator and avoid loading all sentences into a list
    def sentences():
        with open(opts.input) as f:
            for pair in f:
                yield [sentence.strip().split() for sentence in pair.split(' ||| ')]

    # note: the -n option does not work in the original code
    for h1, h2, ref in islice(sentences(), opts.num_sentences):

        # setting sentences to lower case
        lower_h1    = []
        lower_h2    = []
        lower_ref   = []
        for w in h1:
            lower_h1.append(w.lower())
        for s in h2:
            lower_h2.append(s.lower())
        for t in ref:
            lower_ref.append(t.lower())
        rset = set(lower_ref)

        # calculating meteor values
        l_h1 = meteor_calc(lower_h1, rset)
        l_h2 = meteor_calc(lower_h2, rset)

        # calculating bleu values
        b_h1 = bleu_calc(lower_h1, lower_ref)
        b_h2 = bleu_calc(lower_h2, lower_ref)

        # summing the values together
        sum_h1 = l_h1 + b_h1
        sum_h2 = l_h2 + b_h2

        print(1 if sum_h1 > sum_h2 else # \begin{cases}
               (0 if sum_h1 == sum_h2
                   else -1))            # \end{cases}

# convention to allow import of this file as a module
if __name__ == '__main__':
    main()
